<!-- 
Reviewed against source: 2025-12-30
Copyright: 2025 Moonlight Technologies Inc. All Rights Reserved.
Author: Masahiro Aoki
-->


# EvoSpikeNet ダッシュボード ユーザーマニュアル

**最終更新日:** 2025年12月9日

## このドキュメントの目的と使い方
- 目的: ダッシュボード各ページの役割と操作の全体像を素早く掴むための利用ガイド。
- 対象読者: ダッシュボード利用者、運用・検証メンバー、新規参加メンバー。
- まず読む順: 3章「UIナビゲーションとページ機能」→ 必要に応じて 4章の詳細機能説明。
- 関連リンク: 分散脳の実装詳細は [docs/implementation/PFC_ZENOH_EXECUTIVE.md](docs/implementation/PFC_ZENOH_EXECUTIVE.md)、実行スクリプトは `examples/run_zenoh_distributed_brain.py`。

- 実装ノート（アーティファクト）: `docs/implementation/ARTIFACT_MANIFESTS.md` — 学習/出力スクリプトが生成する `artifact_manifest.json` と、フロントエンド/CLI で指定するべきフラグ仕様（`--artifact-name`, `--precision`, `--quantize`, `--privacy-level`, `--node-type`）を参照してください。

## 3. UIナビゲーションとページ機能

ダッシュボードは、画面上部のナビゲーションメニューから各機能にアクセスできます。各ページは機能ごとに整理されています。

### 3.1. Home
このユーザーマニュアルやプロジェクトのREADMEなど、基本的なドキュメントを表示するデフォルトのページです。

### 3.2. Language & MultiModal Models (言語・マルチモーダルモデル)

- **SpikingEvoTextLM**:
    - SNNベースの言語モデル (`SpikingEvoTextLM`) と対話、および学習を行うページです。
- **MultiModal LM**:
    - **Vision-Language**: 画像とテキストを組み合わせたマルチモーダルモデルの学習と推論を行います。
    - **Audio**: 音声データの文字起こし（ASR）や音声モデルの学習を行う機能が統合されています。（旧Audio Tools）
- **Vision Encoder**:
    - 画像認識用のSpiking Vision Encoder単体の学習と推論テストを行います。
- **Audio Encoder**:
    - 音声特徴抽出用のSpiking Audio Encoder単体の学習と推論テストを行います。
- **Text Classifier**:
    - テキスト分類タスク用のSNNモデルを扱います。

### 3.3. RAG & Knowledge (知識検索・データ)

- **RAG System**:
    - ハイブリッド検索（Milvus + Elasticsearch）を用いたRAG（検索拡張生成）チャットを行います。知識ベースの管理もここから行います。
    - **クエリ処理デバッグ機能** ⭐ NEW: 「クエリ処理の詳細を表示」チェックボックスを有効にすると、以下の情報が表示されます：
        - クエリ分析（言語検出、抽出キーワード）
        - ベクトル検索結果（L2距離スコア、文書プレビュー）
        - キーワード検索結果（BM25スコア、文書プレビュー）
        - RRF融合プロセス（ランキング、スコア計算）
        - 生成詳細（コンテキスト長、プロンプトプレビュー、応答タイプ）
    - **知識ベース管理**: 文書の追加・編集・削除、リアルタイム文字数カウンター、バッチ削除機能
- **Data Creation**:
    - シミュレーションや学習に使用するデータの生成・変換を行います。

### 3.4. Advanced Systems (高度なシステム)

- **Distributed Brain**:
    - 分散脳シミュレーションの構成、実行、リアルタイム監視を行うメインコンソールです。
- **Motor Cortex**:
    - ロボット制御のための運動野シミュレーション。模倣学習から強化学習までのパイプラインを管理します。

### 3.5. Tools & Analysis (ツール・分析)

- **Visualization**:
    - 保存されたニューロンデータ（`.pt`ファイル）をアップロードし、スパイク活動やアテンションマップを詳細に可視化します。
- **Model Management** ⭐ UPDATED:
    - データベースに保存されたモデルアーティファクトの管理（一覧、ダウンロード、アップロード）を行います。
    - **モデル分類システム** (2025年12月17日追加):
        - **Brain Node Type (脳ノードタイプ)**: Vision/視覚 (Rank 1), Motor/運動 (Rank 2), Auditory/聴覚 (Rank 5), Speech/音声生成 (Rank 6), Executive/実行制御 (Rank 0), General/汎用
        - **Model Category (モデルカテゴリ)**: 各ノードに対応した20+種類（画像分類、物体検出、運動制御、音声認識、テキスト生成など）
        - **Model Variant (モデルバリアント)**: 軽量版/Lightweight, 標準版/Standard, 高精度版/High Accuracy, リアルタイム版/Realtime, 実験版/Experimental
    - **アップロード時の分類**: モデルアップロード時に、LLMタイプに加えて、ノードタイプ、カテゴリ、バリアントを選択可能。カテゴリはノードタイプに応じて動的にフィルタリングされます。
    - **テーブル表示**: モデル一覧テーブルに分類情報を表示し、フィルタリング可能。
    - **ノードタイプ別推奨モデル**: 各ノードタイプに対する推奨モデルとユースケースを表示。
- **System Utilities**:
    - システム状態の確認、キャッシュクリア、テスト実行などのユーティリティ機能です。
- **Tuning**:
    - `Optuna`を用いたハイパーパラメータ自動最適化の実行と結果分析を行います。


---

## 4. 詳細な機能説明

### 4.1. Distributed Brain (分散脳)

ナビゲーションメニューの「Distributed Brain」ページは、フレームワークの最も高度な機能です。Zenoh版の公式スクリプト `examples/run_zenoh_distributed_brain.py` を介して、複数のプロセス（ノード）で構成される分散脳シミュレーションを管理・実行します（`run_distributed_brain_simulation.py` は後方互換の非推奨ラッパー）。
PFC/Zenoh/ExecutiveControl の実装詳細は [docs/implementation/PFC_ZENOH_EXECUTIVE.md](docs/implementation/PFC_ZENOH_EXECUTIVE.md) を参照してください。

- **Node Configuration (ノード設定) タブ**: シミュレーションのアーキテクチャを定義します。異なる機能モジュール（例：「Language」、「Vision」）を異なるマシン（ローカルまたはSSH経由のリモート）で実行するように割り当て、各ノードに特定の訓練済みモデルを割り当てます。
- **Brain Simulation (脳シミュレーション) タブ**: 実行中のシミュレーションと対話します。マルチモーダルなプロンプト（テキスト、画像、音声）を送信し、通信経路、PFCのエネルギーレベル、個々のノードのログなど、分散システム全体の内部状態をリアルタイムで監視します。

### 4.2. Motor Cortex (運動野)

「Motor Cortex」ページは、高度な4段階の学習パイプラインに基づき、適応的なロボット運動システムを訓練するための完全なワークフローを提供します。これは、`evo_motor_master.py`バックエンドスクリプトによって編成されます。

- **Configuration (設定) タブ**:
    - ここでは、ロボットの物理的なハードウェアをYAMLファイルで定義します。これには、モーターグループ、自由度、関節の可動域制限、安全パラメータの定義が含まれます。この設定は、後続のすべての学習およびシミュレーションステージで使用されます。

- **Learning Pipeline (学習パイプライン) タブ**:
    - これは、エージェントを訓練するためのメインコントロールセンターです。ワークフローは連続したステージに分かれています：
    1.  **ステージ1: 模倣学習**: 人間がタスクを実行するビデオファイルをアップロードしてプロセスを開始します。「Start Stage 1」をクリックすると、ロボットに動きの基本的な理解を与えるための行動クローニングプロセスが実行されます。
    2.  **ステージ2: 実世界強化学習**: ロボットは自己改善フェーズに入ります。高レベルの目標（例：「カップを手に取り、棚に置く」）を与え、「Start Stage 2」をクリックします。ロボットはタスクを繰り返し練習し、強化学習（`SpikePPO`）を用いて、より速く、より滑らかに、より成功率の高い動作を獲得します。
    3.  **ステージ3: ゼロショット汎化**: ステージ2の後、テキストボックスに全く新しいコマンドを入力し、「Attempt New Task」をクリックすることで、ロボットの汎化能力をテストできます。

- **Live Control (ライブ制御) タブ**:
    - 訓練後、このタブを使用して、訓練済みエージェントのライブステータスを監視し、新しいコマンドを発行できます。
    - **ステージ4: 人間協調モード**を有効にすることもでき、ロボットは力覚センサーを使用して人間の動きに反応し、支援します。

- **Monitoring (監視)**:
    - すべてのステージを通じて、「Master Log & Progress」パネルはバックエンドスクリプトからのログをライブストリームで提供します。ステージ2の間、「Learning Progress」グラフはリアルタイムで更新され、エージェントの成功率と報酬の向上を示します。

---

## 5. 生産準備機能 (P3実装完了)

### 5.1. 遅延監視と最適化

システム全体のエンドツーエンド遅延を500ms以内に維持するための監視機能です。

- **遅延統計ダッシュボード**: 全コンポーネントの遅延をリアルタイム監視
- **ターゲットチェック**: 95パーセンタイルが500ms以内の確認
- **パフォーマンス最適化**: ボトルネックの自動検出と改善提案

### 5.2. スナップショット/災害復旧

システムの完全な状態を保存・復旧するための機能です。

- **自動スナップショット**: 定期的なシステム状態の保存
- **圧縮・チェックサム**: データ完全性の保証
- **災害復旧**: 障害発生時の迅速なシステム復旧
- **地理的バックアップ**: 複数拠点へのバックアップ

### 5.3. 大規模スケーラビリティ検証

1000ノード以上の大規模システムでの性能検証機能です。

- **ノードスケーリングテスト**: ノード数増加時の性能測定
- **リソース監視**: CPU/メモリ/ネットワーク使用状況の追跡
- **ストレステスト**: 極限状況でのシステム安定性検証
- **ボトルネック分析**: 性能制限要因の特定

### 5.4. ハードウェア最適化

様々なハードウェアでのモデル実行を最適化する機能です。

- **ONNXエクスポート**: クロスプラットフォーム互換性の確保
- **モデル量子化**: メモリ使用量と推論速度の最適化
- **ハードウェアベンチマーク**: GPU/TPU/ニューロモーフィックチップでの性能測定
- **自動最適化**: ターゲットハードウェアに合わせたモデル調整

### 5.5. 高可用性監視 (99.9%+)

システムの可用性を99.9%以上に維持するための監視機能です。

- **ヘルスチェック**: 全コンポーネントの定期的な健全性確認
- **自動復旧**: 障害検知時の自動回復処理
- **可用性統計**: アップタイム/ダウンタイムの詳細追跡
- **アラートシステム**: 可用性低下時の通知

### 5.6. 非同期Zenoh通信

分散システム間の高性能通信機能です。

- **構造化メッセージング**: 型安全なデータ交換
- **リクエスト/レスポンス**: 同期通信の非同期版
- **Pub/Subパターン**: イベント駆動型通信
- **パフォーマンス監視**: 通信遅延とスループットの追跡

### 5.7. 分散意思決定コンセンサス

複数ノード間での合意形成機能です。

- **Raftアルゴリズム**: リーダー選出とログ複製
- **Paxosアルゴリズム**: 分散合意の代替実装
- **投票システム**: 多数決による決定
- **参加者管理**: ノードの動的追加・削除

---

## 6. APIエンドポイント一覧

### 遅延監視API
- `GET /api/latency/stats` - 遅延統計取得
- `GET /api/latency/check_target` - ターゲット達成確認

### スナップショットAPI
- `POST /api/snapshot/create` - スナップショット作成
- `POST /api/snapshot/restore` - システム復旧
- `GET /api/snapshot/list` - スナップショット一覧

### スケーラビリティAPI
- `POST /api/scalability/test` - スケーラビリティテスト実行
- `GET /api/scalability/resources` - リソース使用状況取得

### ハードウェア最適化API
- `POST /api/hardware/optimize` - モデル最適化
- `POST /api/hardware/benchmark` - ベンチマーク実行
- `POST /api/hardware/export/onnx` - ONNXエクスポート

### 高可用性API
- `GET /api/availability/status` - 可用性ステータス取得
- `POST /api/availability/healthcheck` - ヘルスチェック実行

### Zenoh通信API
- `GET /api/zenoh/stats` - 通信統計取得

### コンセンサスAPI
- `POST /api/consensus/propose` - 決定提案
- `GET /api/consensus/result/{id}` - 結果取得
- `GET /api/consensus/stats` - コンセンサス統計

これらの機能により、EvoSpikeNetは本番環境での使用に必要なすべての高度な機能を備えています。
