# Copyright 2025 Moonlight Technologies Inc. All Rights Reserved.
# Auth Masahiro Aoki


# EvoSpikeNet ダッシュボード ユーザーマニュアル

**最終更新日:** 2025年12月9日

## 3. UIナビゲーションとページ機能

ダッシュボードは、画面上部のナビゲーションメニューから各機能にアクセスできます。各ページは機能ごとに整理されています。

### 3.1. Home
このユーザーマニュアルやプロジェクトのREADMEなど、基本的なドキュメントを表示するデフォルトのページです。

### 3.2. Language & MultiModal Models (言語・マルチモーダルモデル)

- **SpikingEvoTextLM**:
    - SNNベースの言語モデル (`SpikingEvoTextLM`) と対話、および学習を行うページです。
- **MultiModal LM**:
    - **Vision-Language**: 画像とテキストを組み合わせたマルチモーダルモデルの学習と推論を行います。
    - **Audio**: 音声データの文字起こし（ASR）や音声モデルの学習を行う機能が統合されています。（旧Audio Tools）
- **Vision Encoder**:
    - 画像認識用のSpiking Vision Encoder単体の学習と推論テストを行います。
- **Audio Encoder**:
    - 音声特徴抽出用のSpiking Audio Encoder単体の学習と推論テストを行います。
- **Text Classifier**:
    - テキスト分類タスク用のSNNモデルを扱います。

### 3.3. RAG & Knowledge (知識検索・データ)

- **RAG System**:
    - ハイブリッド検索（Milvus + Elasticsearch）を用いたRAG（検索拡張生成）チャットを行います。知識ベースの管理もここから行います。
- **Data Creation**:
    - シミュレーションや学習に使用するデータの生成・変換を行います。

### 3.4. Advanced Systems (高度なシステム)

- **Distributed Brain**:
    - 分散脳シミュレーションの構成、実行、リアルタイム監視を行うメインコンソールです。
- **Motor Cortex**:
    - ロボット制御のための運動野シミュレーション。模倣学習から強化学習までのパイプラインを管理します。

### 3.5. Tools & Analysis (ツール・分析)

- **Visualization**:
    - 保存されたニューロンデータ（`.pt`ファイル）をアップロードし、スパイク活動やアテンションマップを詳細に可視化します。
- **Model Management**:
    - データベースに保存されたモデルアーティファクトの管理（一覧、ダウンロード、アップロード）を行います。
- **System Utilities**:
    - システム状態の確認、キャッシュクリア、テスト実行などのユーティリティ機能です。
- **Tuning**:
    - `Optuna`を用いたハイパーパラメータ自動最適化の実行と結果分析を行います。


---

## 4. 詳細な機能説明

### 4.1. Distributed Brain (分散脳)

ナビゲーションメニューの「Distributed Brain」ページは、フレームワークの最も高度な機能です。`run_distributed_brain_simulation.py`スクリプトを介して、複数のプロセス（ノード）で構成される分散脳シミュレーションを管理・実行します。

- **Node Configuration (ノード設定) タブ**: シミュレーションのアーキテクチャを定義します。異なる機能モジュール（例：「Language」、「Vision」）を異なるマシン（ローカルまたはSSH経由のリモート）で実行するように割り当て、各ノードに特定の訓練済みモデルを割り当てます。
- **Brain Simulation (脳シミュレーション) タブ**: 実行中のシミュレーションと対話します。マルチモーダルなプロンプト（テキスト、画像、音声）を送信し、通信経路、PFCのエネルギーレベル、個々のノードのログなど、分散システム全体の内部状態をリアルタイムで監視します。

### 4.2. Motor Cortex (運動野)

「Motor Cortex」ページは、高度な4段階の学習パイプラインに基づき、適応的なロボット運動システムを訓練するための完全なワークフローを提供します。これは、`evo_motor_master.py`バックエンドスクリプトによって編成されます。

- **Configuration (設定) タブ**:
    - ここでは、ロボットの物理的なハードウェアをYAMLファイルで定義します。これには、モーターグループ、自由度、関節の可動域制限、安全パラメータの定義が含まれます。この設定は、後続のすべての学習およびシミュレーションステージで使用されます。

- **Learning Pipeline (学習パイプライン) タブ**:
    - これは、エージェントを訓練するためのメインコントロールセンターです。ワークフローは連続したステージに分かれています：
    1.  **ステージ1: 模倣学習**: 人間がタスクを実行するビデオファイルをアップロードしてプロセスを開始します。「Start Stage 1」をクリックすると、ロボットに動きの基本的な理解を与えるための行動クローニングプロセスが実行されます。
    2.  **ステージ2: 実世界強化学習**: ロボットは自己改善フェーズに入ります。高レベルの目標（例：「カップを手に取り、棚に置く」）を与え、「Start Stage 2」をクリックします。ロボットはタスクを繰り返し練習し、強化学習（`SpikePPO`）を用いて、より速く、より滑らかに、より成功率の高い動作を獲得します。
    3.  **ステージ3: ゼロショット汎化**: ステージ2の後、テキストボックスに全く新しいコマンドを入力し、「Attempt New Task」をクリックすることで、ロボットの汎化能力をテストできます。

- **Live Control (ライブ制御) タブ**:
    - 訓練後、このタブを使用して、訓練済みエージェントのライブステータスを監視し、新しいコマンドを発行できます。
    - **ステージ4: 人間協調モード**を有効にすることもでき、ロボットは力覚センサーを使用して人間の動きに反応し、支援します。

- **Monitoring (監視)**:
    - すべてのステージを通じて、「Master Log & Progress」パネルはバックエンドスクリプトからのログをライブストリームで提供します。ステージ2の間、「Learning Progress」グラフはリアルタイムで更新され、エージェントの成功率と報酬の向上を示します。
