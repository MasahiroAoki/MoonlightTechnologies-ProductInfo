<!-- Reviewed against source: 2025-12-21. English translation pending. -->
# Copyright 2025 Moonlight Technologies Inc. All Rights Reserved.
# Auth Masahiro Aoki

# åˆ†æ•£è„³ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹LLMçµ±åˆæˆ¦ç•¥ã®æ¯”è¼ƒæ¤œè¨

**ä½œæˆæ—¥**: 2025-12-06  
**å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: EvoSpikeNet Zenohåˆ†æ•£è„³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

## ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç›®çš„ã¨ä½¿ã„æ–¹
- ç›®çš„: LLMçµ±åˆã®æ–¹é‡ï¼ˆçµ±åˆå‹ vs åˆ†æ•£å‹ï¼‰ã‚’æ¯”è¼ƒã—ã€å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã®åˆ¤æ–­ææ–™ã‚’æä¾›ã™ã‚‹ã€‚
- å¯¾è±¡èª­è€…: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã€LLM/åˆ†æ•£æ‹…å½“ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€PMã€‚
- ã¾ãšèª­ã‚€é †: å®Ÿè¡Œã‚µãƒãƒªãƒ¼ â†’ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¯”è¼ƒ â†’ æ¨å¥¨æˆ¦ç•¥ â†’ å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã€‚
- é–¢é€£ãƒªãƒ³ã‚¯: åˆ†æ•£è„³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ `examples/run_zenoh_distributed_brain.py`ã€PFC/Zenoh/Executiveè©³ç´°ã¯ [implementation/PFC_ZENOH_EXECUTIVE.md](implementation/PFC_ZENOH_EXECUTIVE.md)ã€‚
 - å®Ÿè£…ãƒãƒ¼ãƒˆï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼‰: `docs/implementation/ARTIFACT_MANIFESTS.md` â€” å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç”Ÿæˆã™ã‚‹ `artifact_manifest.json` ã¨ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰/CLI ãƒ•ãƒ©ã‚°ã®ä»•æ§˜ã«ã¤ã„ã¦ã€‚

## å®Ÿè¡Œã‚µãƒãƒªãƒ¼

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€åˆ†æ•£è„³ã‚·ã‚¹ãƒ†ãƒ ã¸ã®LLMçµ±åˆã«ãŠã„ã¦ã€ä»¥ä¸‹ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©³ç´°ã«æ¯”è¼ƒæ¤œè¨ã—ã¾ã™:

1. **çµ±åˆå‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: å˜ä¸€ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMï¼ˆ`SpikingMultiModalLM`ï¼‰ã‚’ã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ
2. **åˆ†æ•£å‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: å„ãƒãƒ¼ãƒ‰ã«ç‰¹åŒ–ã—ãŸLLMã‚’å€‹åˆ¥ã«ä½œæˆã—ã€ãƒªãƒ¢ãƒ¼ãƒˆPCã§ç‹¬ç«‹ãƒ­ãƒ¼ãƒ‰

## ç›®æ¬¡

1. [ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³](#ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³)
2. [ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¯”è¼ƒ](#ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¯”è¼ƒ)
3. [è©³ç´°åˆ†æ](#è©³ç´°åˆ†æ)
4. [æ¨å¥¨æˆ¦ç•¥](#æ¨å¥¨æˆ¦ç•¥)
5. [å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—](#å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—)

---

## ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³

### æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### 1. SpikingMultiModalLMï¼ˆçµ±åˆå‹ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `evospikenet/models.py:275-381`

```python
class SpikingMultiModalLM(nn.Module):
    """
    çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«SNNè¨€èªãƒ¢ãƒ‡ãƒ«
    ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã‚’çµ±åˆå‡¦ç†
    
    Note: Previously named MultiModalEvoSpikeNetLM (deprecated).
    """
    def __init__(self,
                 vocab_size: int,
                 d_model: int,
                 n_heads: int,
                 num_transformer_blocks: int,
                 time_steps: int,
                 image_input_channels: int = 1,
                 audio_input_features: int = 13):
        
        # å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.text_encoder = TASEncoderDecoder(...)
        self.vision_encoder = SpikingEvoVisionEncoder(
            input_channels=image_input_channels,
            output_dim=d_model,
            time_steps=time_steps,
            image_size=(28, 28)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯MNISTã‚µã‚¤ã‚º
        )
        self.audio_encoder = SpikingAudioEncoder(...)
        
        # ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³å±¤ï¼ˆ3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµåˆï¼‰
        self.fusion_layer = nn.Linear(d_model * 3, d_model)
        
        # å…±æœ‰ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        self.transformer_blocks = nn.ModuleList([...])
```

**ç‰¹å¾´**:
- âœ… 3ã¤ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ï¼‰ã‚’çµ±åˆ
- âœ… ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³å±¤ã§ç‰¹å¾´ã‚’èåˆ
- âœ… å…±æœ‰ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’
- âŒ å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å¸¸ã«å«ã‚€ï¼ˆãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰

#### 2. å€‹åˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆåˆ†æ•£å‹å€™è£œï¼‰

##### SpikingEvoVisionEncoder (æ—§å: SpikingVisionEncoder)
**ãƒ•ã‚¡ã‚¤ãƒ«**: `evospikenet/vision.py:14-105`

```python
class SpikingEvoVisionEncoder(nn.Module):
    """ç”»åƒâ†’ã‚¹ãƒ‘ã‚¤ã‚¯å¤‰æ›ç‰¹åŒ–
    
    Spiking CNNã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã€‚ç”»åƒã‚’æ™‚ç³»åˆ—ã‚¹ãƒ‘ã‚¤ã‚¯åˆ—ã«å¤‰æ›ã—ã¾ã™ã€‚
    """
    def __init__(self, input_channels: int = 1, 
                 output_dim: int = 64, 
                 time_steps: int = 20,
                 image_size: tuple = (28, 28)):  # âœ… è¿½åŠ 
        self.conv1 = nn.Conv2d(input_channels, 12, kernel_size=5)
        self.conv2 = nn.Conv2d(12, 32, kernel_size=5)
        # fc1ã¯image_sizeã‹ã‚‰è¨ˆç®—ã•ã‚ŒãŸflat_dimã§åˆæœŸåŒ–
        # ä¾‹: MNIST (28x28) â†’ flat_dim = 32 * 2 * 2 = 128
        #     CIFAR-10 (32x32) â†’ flat_dim = 32 * 3 * 3 = 288
        self.fc1 = nn.Linear(flat_dim, output_dim)
        # LIFå±¤...
```

**ä½¿ç”¨ä¾‹**:
```python
# MNISTç”¨ï¼ˆ28x28ã€ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
encoder_mnist = SpikingEvoVisionEncoder(
    input_channels=1, 
    output_dim=64, 
    time_steps=20,
    image_size=(28, 28)
)

# CIFAR-10ç”¨ï¼ˆ32x32ã€RGBï¼‰
encoder_cifar = SpikingEvoVisionEncoder(
    input_channels=3, 
    output_dim=128, 
    time_steps=20,
    image_size=(32, 32)
)
```

**æ³¨æ„**: æ—§å`SpikingVisionEncoder`ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã•ã‚Œã¦ã„ã¾ã™ãŒã€v2.0ã§å‰Šé™¤äºˆå®šã§ã™ã€‚


**ç‰¹å¾´**:
- âœ… è»½é‡ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ~50Kï¼‰
- âœ… è¦–è¦šå‡¦ç†ã«ç‰¹åŒ–
- âœ… ç‹¬ç«‹ã—ã¦å‹•ä½œå¯èƒ½

##### SpikingAudioEncoder
**ãƒ•ã‚¡ã‚¤ãƒ«**: `evospikenet/audio.py:25-57`

```python
class SpikingAudioEncoder(nn.Module):
    """MFCCâ†’ã‚¹ãƒ‘ã‚¤ã‚¯å¤‰æ›ç‰¹åŒ–"""
    def __init__(self, input_features, output_neurons, time_steps):
        self.fc = nn.Linear(input_features, output_neurons)
        self.lif = snn.Leaky(...)
```

**ç‰¹å¾´**:
- âœ… è¶…è»½é‡ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ~10Kï¼‰
- âœ… éŸ³å£°å‡¦ç†ã«ç‰¹åŒ–
- âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å‘ã‘

### ç¾åœ¨ã®åˆ†æ•£ãƒãƒ¼ãƒ‰æ§‹æˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `examples/run_zenoh_distributed_brain.py:697-702`

```python
node_configs = [
    ("pfc-0", "pfc", 0, {"d_model": 256}),           # PFC: èª¿æ•´å½¹
    ("visual-0", "visual", 1, {"d_model": 128}),     # Visual: è¦–è¦šå‡¦ç†
    ("motor-0", "motor", 1, {"d_model": 128}),       # Motor: é‹å‹•åˆ¶å¾¡
    ("lang-main", "lang-main", 0, {"d_model": 128}) # Lang: è¨€èªç”Ÿæˆ
]
```

**ç¾çŠ¶ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰**:
- **Lang-Main**: `SpikingEvoSpikeNetLM`ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
- **Visual/Motor/PFC**: `SimpleLIFNode`ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªLIFå±¤ã®ã¿ï¼‰

---

## ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¯”è¼ƒ

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: çµ±åˆå‹ï¼ˆå˜ä¸€MultiModalEvoSpikeNetLMï¼‰

```mermaid
graph TB
    subgraph "Lang-Main Node: ãƒªãƒ¢ãƒ¼ãƒˆPC1"
        MM["MultiModalEvoSpikeNetLM: 256MB"]
        TE["Text Encoder"]
        VE["Vision Encoder"]
        AE["Audio Encoder"]
        FL["Fusion Layer"]
        TB["Transformer Blocks x N"]
        
        MM --> TE
        MM --> VE
        MM --> AE
        TE --> FL
        VE --> FL
        AE --> FL
        FL --> TB
    end
    
    subgraph "Visual Node: ãƒªãƒ¢ãƒ¼ãƒˆPC2"
        VS["SimpleLIFNode: 1MB"]
    end
    
    subgraph "Audio Node: ãƒªãƒ¢ãƒ¼ãƒˆPC3"
        AS["SimpleLIFNode: 1MB"]
    end
    
    VS -->|"Zenoh: Spikes"| MM
    AS -->|"Zenoh: Spikes"| MM
```

#### ãƒ¡ãƒªãƒƒãƒˆ

| é …ç›®                     | èª¬æ˜                                  | é‡è¦åº¦ |
| ------------------------ | ------------------------------------- | ------ |
| **ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’**   | å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã§æ³¨æ„æ©Ÿæ§‹ãŒåƒã        | ğŸ”´ æœ€é«˜ |
| **çµ±ä¸€çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ** | å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§å…¨æƒ…å ±ã‚’çµ±åˆç†è§£          | ğŸ”´ æœ€é«˜ |
| **å®Ÿè£…ã®ç°¡æ½”ã•**         | æ—¢å­˜ã®`MultiModalEvoSpikeNetLM`ã‚’æ´»ç”¨ | ğŸŸ¡ é«˜   |
| **å­¦ç¿’åŠ¹ç‡**             | ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã§æ±åŒ–æ€§èƒ½å‘ä¸Š        | ğŸŸ¡ é«˜   |
| **ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹**         | å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç†ã®ã¿                  | ğŸŸ¢ ä¸­   |

#### ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| é …ç›®                 | èª¬æ˜                                    | å½±éŸ¿åº¦ |
| -------------------- | --------------------------------------- | ------ |
| **ãƒ¡ãƒ¢ãƒªæ¶ˆè²»**       | Lang-Mainãƒãƒ¼ãƒ‰ã«å…¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å¸¸é§   | ğŸ”´ æœ€é«˜ |
| **è¨ˆç®—è² è·é›†ä¸­**     | å˜ä¸€ãƒãƒ¼ãƒ‰ã«å‡¦ç†ãŒé›†ä¸­                  | ğŸ”´ æœ€é«˜ |
| **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**     | Lang-Mainãƒãƒ¼ãƒ‰ãŒã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ€§èƒ½ä¸Šé™ | ğŸ”´ æœ€é«˜ |
| **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** | ãƒãƒ¼ãƒ‰è¿½åŠ æ™‚ã«å…¨ãƒ¢ãƒ‡ãƒ«å†é…å¸ƒãŒå¿…è¦      | ğŸŸ¡ é«˜   |
| **å†—é•·æ€§**           | ä½¿ã‚ãªã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚‚å¸¸ã«ãƒ¡ãƒ¢ãƒªã«å¸¸é§  | ğŸŸ¡ é«˜   |

#### ãƒªã‚½ãƒ¼ã‚¹è¦‹ç©ã‚‚ã‚Š

```python
# MultiModalEvoSpikeNetLMã®æ¨å®šã‚µã‚¤ã‚ºï¼ˆd_model=128ã®å ´åˆï¼‰
component_sizes = {
    "text_encoder": 20_000_000,      # 20M params
    "vision_encoder": 50_000,        # 50K params
    "audio_encoder": 10_000,         # 10K params
    "fusion_layer": 49_152,          # 128*3 -> 128
    "transformer_blocks": 80_000_000, # 80M params (4 blocks)
    "output_fc": 3_865_344           # 128 -> 30522 (vocab)
}

total_params = sum(component_sizes.values())  # ~104M params
memory_fp32 = total_params * 4 / (1024**2)   # ~416 MB
memory_fp16 = total_params * 2 / (1024**2)   # ~208 MB
```

**Lang-Mainãƒãƒ¼ãƒ‰è¦ä»¶**:
- RAM: æœ€ä½ **2GB**ï¼ˆFP16ä½¿ç”¨æ™‚ï¼‰
- GPU VRAM: æœ€ä½ **4GB**ï¼ˆæ¨è«–æ™‚ï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: 100Mbpsä»¥ä¸Šï¼ˆåˆå›ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ï¼‰

---

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: åˆ†æ•£å‹ï¼ˆå„ãƒãƒ¼ãƒ‰ç‰¹åŒ–LLMï¼‰

```mermaid
graph TB
    subgraph "Lang-Main Node: ãƒªãƒ¢ãƒ¼ãƒˆPC1"
        TLM["SpikingTextLM: 80MB"]
    end
    
    subgraph "Visual Node: ãƒªãƒ¢ãƒ¼ãƒˆPC2"
        VLM["SpikingVisionLM: 150MB"]
        VE2["Vision Encoder"]
        VT["Vision Transformer"]
        VLM --> VE2
        VE2 --> VT
    end
    
    subgraph "Audio Node: ãƒªãƒ¢ãƒ¼ãƒˆPC3"
        ALM["SpikingAudioLM: 100MB"]
        AE2["Audio Encoder"]
        AT["Audio Transformer"]
        ALM --> AE2
        AE2 --> AT
    end
    
    subgraph "PFC Node: ãƒªãƒ¢ãƒ¼ãƒˆPC4"
        PFC["PFCDecisionEngine: 50MB"]
        QM["QuantumModulation"]
        PFC --> QM
    end
    
    VLM -->|"Zenoh: High-level Features"| TLM
    ALM -->|"Zenoh: High-level Features"| TLM
    PFC -->|"Zenoh: Routing"| VLM
    PFC -->|"Zenoh: Routing"| ALM
    PFC -->|"Zenoh: Routing"| TLM
```

#### ãƒ¡ãƒªãƒƒãƒˆ

| é …ç›®                 | èª¬æ˜                                   | é‡è¦åº¦ |
| -------------------- | -------------------------------------- | ------ |
| **åˆ†æ•£å‡¦ç†**         | å„ãƒãƒ¼ãƒ‰ãŒç‹¬ç«‹ã—ã¦å‡¦ç†ãƒ»æœ€é©åŒ–         | ğŸ”´ æœ€é«˜ |
| **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** | ãƒãƒ¼ãƒ‰è¿½åŠ ãŒå®¹æ˜“ã€æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½     | ğŸ”´ æœ€é«˜ |
| **å°‚é–€åŒ–**           | å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«æœ€é©åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | ğŸ”´ æœ€é«˜ |
| **éšœå®³è€æ€§**         | 1ãƒãƒ¼ãƒ‰éšœå®³ã§ã‚‚ä»–ãƒãƒ¼ãƒ‰ç¶™ç¶šå‹•ä½œ        | ğŸŸ¡ é«˜   |
| **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**       | å„ãƒãƒ¼ãƒ‰ã¯å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰       | ğŸŸ¡ é«˜   |
| **ä¸¦åˆ—å‡¦ç†**         | è¤‡æ•°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çœŸã«ä¸¦åˆ—å‡¦ç†å¯èƒ½       | ğŸŸ¡ é«˜   |
| **é–‹ç™ºæŸ”è»Ÿæ€§**       | å„ãƒ¢ãƒ‡ãƒ«ã‚’ç‹¬ç«‹ã—ã¦æ”¹å–„å¯èƒ½             | ğŸŸ¢ ä¸­   |

#### ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| é …ç›®                   | èª¬æ˜                             | å½±éŸ¿åº¦ |
| ---------------------- | -------------------------------- | ------ |
| **å®Ÿè£…è¤‡é›‘åº¦**         | æ–°è¦ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆãƒ»å®Ÿè£…ãŒå¿…è¦       | ğŸ”´ æœ€é«˜ |
| **é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** | ãƒãƒ¼ãƒ‰é–“ã§é«˜æ¬¡ç‰¹å¾´é‡ã‚’é »ç¹ã«é€ä¿¡ | ğŸŸ¡ é«˜   |
| **å­¦ç¿’ã®è¤‡é›‘ã•**       | å„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æˆ¦ç•¥ã‚’å€‹åˆ¥è¨­è¨ˆ     | ğŸŸ¡ é«˜   |
| **çµ±åˆã®é›£ã—ã•**       | ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã®å®Ÿè£…ãŒè¤‡é›‘   | ğŸŸ¡ é«˜   |
| **ä¸€è²«æ€§ç®¡ç†**         | å„ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãŒå¿…è¦   | ğŸŸ¢ ä¸­   |

#### ãƒªã‚½ãƒ¼ã‚¹è¦‹ç©ã‚‚ã‚Š

```python
# å„ãƒãƒ¼ãƒ‰ã®æ¨å®šãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
node_model_sizes = {
    "lang-main": {
        "model": "SpikingTextLM",
        "params": 80_000_000,      # 80M params
        "memory_fp16": 160          # MB
    },
    "visual": {
        "model": "SpikingVisionLM",
        "params": 150_000_000,     # 150M params (Vision Transformer)
        "memory_fp16": 300          # MB
    },
    "audio": {
        "model": "SpikingAudioLM",
        "params": 100_000_000,     # 100M params
        "memory_fp16": 200          # MB
    },
    "pfc": {
        "model": "PFCDecisionEngine",
        "params": 50_000_000,      # 50M params
        "memory_fp16": 100          # MB
    }
}

# ç·ãƒ¡ãƒ¢ãƒª: 760 MBï¼ˆå…¨ãƒãƒ¼ãƒ‰åˆè¨ˆï¼‰
# ãŸã ã—ã€å„ãƒãƒ¼ãƒ‰ã¯ç‹¬ç«‹ã—ãŸãƒã‚·ãƒ³ã§å‹•ä½œ
```

**å„ãƒãƒ¼ãƒ‰è¦ä»¶**:
- **Lang-Main**: RAM 1GB, GPU VRAM 2GB
- **Visual**: RAM 1.5GB, GPU VRAM 3GB
- **Audio**: RAM 1GB, GPU VRAM 2.5GB
- **PFC**: RAM 512MB, CPUå¯ï¼ˆè»½é‡ï¼‰

---

## è©³ç´°åˆ†æ

### 1. æ€§èƒ½æ¯”è¼ƒ

#### ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†æ

**çµ±åˆå‹ï¼ˆMultiModalEvoSpikeNetLMï¼‰**:

```
å…¥åŠ›å—ä¿¡ â†’ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° â†’ ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ â†’ Transformer â†’ å‡ºåŠ›
  10ms        50ms              20ms          100ms       10ms
                                                          
ç·ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 190msï¼ˆå˜ä¸€ãƒãƒ¼ãƒ‰å†…å‡¦ç†ï¼‰
```

**åˆ†æ•£å‹ï¼ˆç‰¹åŒ–LLMsï¼‰**:

```
[Visual Node] ç”»åƒå—ä¿¡ â†’ Visionå‡¦ç† â†’ ç‰¹å¾´æŠ½å‡º
                10ms       80ms        20ms
                                        â†“ Zenoh (5ms)
                                        â†“
[PFC Node]    ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®š (10ms) â†’
                                        â†“
[Lang Node]   ãƒ†ã‚­ã‚¹ãƒˆå—ä¿¡ â†’ Langå‡¦ç† â†’ å‡ºåŠ›
                5ms          60ms      10ms

ç·ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 200msï¼ˆåˆ†æ•£å‡¦ç† + é€šä¿¡ï¼‰
```

**çµè«–**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯ã»ã¼åŒç­‰ã€‚åˆ†æ•£å‹ã¯é€šä¿¡ã‚³ã‚¹ãƒˆãŒã‚ã‚‹ãŒã€ä¸¦åˆ—å‡¦ç†ã§ç›¸æ®ºã€‚

#### ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆåˆ†æ

| æŒ‡æ¨™              | çµ±åˆå‹   | åˆ†æ•£å‹               |
| ----------------- | -------- | -------------------- |
| ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡¦ç†  | 50 req/s | 80 req/sï¼ˆLangç‰¹åŒ–ï¼‰ |
| ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† | 20 req/s | 25 req/sï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ |
| 3ãƒ¢ãƒ€ãƒªãƒ†ã‚£åŒæ™‚   | 10 req/s | 30 req/sï¼ˆå®Œå…¨ä¸¦åˆ—ï¼‰ |

**çµè«–**: åˆ†æ•£å‹ã¯è¤‡é›‘ã‚¿ã‚¹ã‚¯ã§2-3å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€‚

### 2. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ

#### ãƒãƒ¼ãƒ‰è¿½åŠ ã‚·ãƒŠãƒªã‚ª

**çµ±åˆå‹**:
```
æ–°è¦Vision Nodeã‚’è¿½åŠ 
 â†’ Lang-Mainã®MultiModalLLMã¯å¤‰æ›´ä¸è¦
 â†’ ãŸã ã—ã€å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯æ—¢ã«å¸¸é§
 â†’ ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®æ©æµã¯é™å®šçš„
```

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: Lang-Mainãƒãƒ¼ãƒ‰ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ã¾ã¾

**åˆ†æ•£å‹**:
```
æ–°è¦Vision Nodeã‚’è¿½åŠ 
 â†’ ç‹¬è‡ªã®SpikingVisionLMã‚’ãƒ­ãƒ¼ãƒ‰
 â†’ PFCãŒè‡ªå‹•çš„ã«æ–°ãƒãƒ¼ãƒ‰ã‚’ç™ºè¦‹ãƒ»ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
 â†’ è¦–è¦šå‡¦ç†èƒ½åŠ›ãŒç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒ«
```

**ãƒ¡ãƒªãƒƒãƒˆ**: çœŸã®æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½

#### ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³å±•é–‹

**çµ±åˆå‹**:
```
[æ±äº¬DC] Lang-Main (MultiModalLM) â† ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
    â†‘
    â””â”€â”€ [å¤§é˜ªDC] Visual Nodes (è¤‡æ•°)
```

**å•é¡Œ**: æ±äº¬-å¤§é˜ªé–“ã®é•·è·é›¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå…¨ä½“ã«å½±éŸ¿

**åˆ†æ•£å‹**:
```
[æ±äº¬DC] 
  - Lang-Main (TextLM)
  - Visual-1 (VisionLM)

[å¤§é˜ªDC]
  - Visual-2 (VisionLM)
  - Audio-1 (AudioLM)
```

**åˆ©ç‚¹**: ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å†…ã§å‡¦ç†å®Œçµã€å¿…è¦æ™‚ã®ã¿ã‚¯ãƒ­ã‚¹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³é€šä¿¡

### 3. é–‹ç™ºãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§

#### å®Ÿè£…ã‚³ã‚¹ãƒˆ

| ãƒ•ã‚§ãƒ¼ã‚º         | çµ±åˆå‹                    | åˆ†æ•£å‹                      |
| ---------------- | ------------------------- | --------------------------- |
| åˆæœŸå®Ÿè£…         | âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«æ´»ç”¨ï¼ˆ1é€±é–“ï¼‰ | âš ï¸ æ–°è¦ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆï¼ˆ4-6é€±é–“ï¼‰ |
| å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | âœ… æ—¢å­˜ä½¿ç”¨å¯èƒ½            | âš ï¸ å„ãƒ¢ãƒ‡ãƒ«å€‹åˆ¥è¨­è¨ˆ          |
| ãƒ†ã‚¹ãƒˆ           | ğŸŸ¢ å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ        | ğŸŸ¡ ãƒãƒ«ãƒãƒãƒ¼ãƒ‰çµ±åˆãƒ†ã‚¹ãƒˆ    |
| ãƒ‡ãƒ—ãƒ­ã‚¤         | ğŸŸ¢ å˜ä¸€ãƒ¢ãƒ‡ãƒ«é…å¸ƒ          | ğŸŸ¡ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ç®¡ç†            |

**åˆæœŸé–‹ç™º**: çµ±åˆå‹ãŒæœ‰åˆ©ï¼ˆæ—¢å­˜è³‡ç”£æ´»ç”¨ï¼‰

#### é•·æœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

| ã‚¿ã‚¹ã‚¯           | çµ±åˆå‹               | åˆ†æ•£å‹                 |
| ---------------- | -------------------- | ---------------------- |
| ãƒ¢ãƒ‡ãƒ«æ”¹å–„       | âš ï¸ å…¨ä½“å†å­¦ç¿’å¿…è¦     | âœ… è©²å½“ãƒãƒ¼ãƒ‰ã®ã¿æ›´æ–°   |
| ãƒã‚°ä¿®æ­£         | âš ï¸ å…¨ãƒãƒ¼ãƒ‰å½±éŸ¿       | âœ… è©²å½“ãƒãƒ¼ãƒ‰ã®ã¿å½±éŸ¿   |
| æ–°ãƒ¢ãƒ€ãƒªãƒ†ã‚£è¿½åŠ  | âš ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ | âœ… æ–°ãƒãƒ¼ãƒ‰è¿½åŠ ã®ã¿     |
| A/Bãƒ†ã‚¹ãƒˆ        | å›°é›£                 | âœ… ãƒãƒ¼ãƒ‰å˜ä½ã§å®Ÿæ–½å¯èƒ½ |

**é•·æœŸé‹ç”¨**: åˆ†æ•£å‹ãŒæœ‰åˆ©ï¼ˆæŸ”è»Ÿæ€§ãƒ»ä¿å®ˆæ€§ï¼‰

### 4. å®Ÿä¸–ç•Œãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹è©•ä¾¡

#### ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹1: ãƒ­ãƒœãƒƒãƒˆçŸ¥è¦šã‚·ã‚¹ãƒ†ãƒ ï¼ˆ2026å¹´é‡ç”£ãƒ­ãƒœãƒƒãƒˆï¼‰

**è¦ä»¶**:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦–è¦šå‡¦ç†ï¼ˆ30fpsï¼‰
- éŸ³å£°ã‚³ãƒãƒ³ãƒ‰èªè­˜
- è¤‡æ•°ãƒ­ãƒœãƒƒãƒˆå”èª¿

**è©•ä¾¡**:

| é …ç›®             | çµ±åˆå‹                    | åˆ†æ•£å‹               |
| ---------------- | ------------------------- | -------------------- |
| ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§   | ğŸŸ¡ Lang-MainãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ | âœ… å„ãƒãƒ¼ãƒ‰ãŒä¸¦åˆ—å‡¦ç† |
| ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | âŒ ãƒ­ãƒœãƒƒãƒˆå¢—åŠ ã§æ€§èƒ½åŠ£åŒ–  | âœ… ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«       |
| éšœå®³è€æ€§         | âŒ å˜ä¸€éšœå®³ç‚¹              | âœ… å†—é•·æ§‹æˆå¯èƒ½       |

**æ¨å¥¨**: ğŸ”´ **åˆ†æ•£å‹**

#### ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹2: ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆå¤§å­¦ç ”ç©¶å®¤ï¼‰

**è¦ä»¶**:
- è¿…é€Ÿãªå®Ÿé¨“ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- é™ã‚‰ã‚ŒãŸãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒªã‚½ãƒ¼ã‚¹
- ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã®ç ”ç©¶

**è©•ä¾¡**:

| é …ç›®         | çµ±åˆå‹             | åˆ†æ•£å‹                 |
| ------------ | ------------------ | ---------------------- |
| å®Ÿè£…é€Ÿåº¦     | âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«å³åˆ©ç”¨ | âš ï¸ æ–°è¦å®Ÿè£…å¿…è¦         |
| ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ | ğŸŸ¡ å˜ä¸€GPUå¿…è¦      | âœ… è¤‡æ•°ä½ã‚¹ãƒšãƒƒã‚¯PCåˆ†æ•£ |
| ç ”ç©¶æŸ”è»Ÿæ€§   | âœ… çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ã§å®Ÿé¨“ | ğŸŸ¡ å„ãƒ¢ãƒ‡ãƒ«å€‹åˆ¥èª¿æ•´     |

**æ¨å¥¨**: ğŸŸ¢ **çµ±åˆå‹**ï¼ˆçŸ­æœŸï¼‰â†’ ğŸ”´ **åˆ†æ•£å‹**ï¼ˆé•·æœŸï¼‰

#### ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹3: ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ï¼ˆIoT/ã‚¹ãƒãƒ¼ãƒˆãƒ›ãƒ¼ãƒ ï¼‰

**è¦ä»¶**:
- ä½æ¶ˆè²»é›»åŠ›
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ–­ç¶šçš„
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼é‡è¦–ï¼ˆã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹å‡¦ç†ï¼‰

**è©•ä¾¡**:

| é …ç›®           | çµ±åˆå‹               | åˆ†æ•£å‹                 |
| -------------- | -------------------- | ---------------------- |
| é›»åŠ›åŠ¹ç‡       | âŒ å…¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å¸¸é§ | âœ… å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã®ã¿     |
| ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œ | ğŸŸ¡ 1ãƒ‡ãƒã‚¤ã‚¹ã§å®Œçµ    | âœ… å„ãƒ‡ãƒã‚¤ã‚¹ãŒè‡ªå¾‹å‹•ä½œ |
| ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼   | ğŸŸ¡ ä¸­å¤®é›†ç´„å‡¦ç†       | âœ… ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†å¯èƒ½     |

**æ¨å¥¨**: ğŸ”´ **åˆ†æ•£å‹**

---

## æ¨å¥¨æˆ¦ç•¥

### æ®µéšçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæ¨å¥¨ï¼‰

æœ€é©è§£ã¯ã€**æ®µéšçš„ã«çµ±åˆå‹ã‹ã‚‰åˆ†æ•£å‹ã¸ç§»è¡Œ**ã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã§ã™ã€‚

```mermaid
gantt
    title LLMçµ±åˆãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
    dateFormat YYYY-MM
    section Phase 1: çµ±åˆå‹
    MultiModalLLMå®Ÿè£…      :done, p1, 2025-12, 1M
    åˆæœŸçµ±åˆãƒ†ã‚¹ãƒˆ         :done, p2, 2026-01, 2w
    section Phase 2: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    Visualç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«é–‹ç™º   :active, p3, 2026-01, 1.5M
    Audioç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«é–‹ç™º    :p4, 2026-02, 1M
    éƒ¨åˆ†çš„åˆ†æ•£åŒ–           :p5, 2026-03, 1M
    section Phase 3: å®Œå…¨åˆ†æ•£
    PFCçµ±åˆå¼·åŒ–            :p6, 2026-04, 1M
    å®Œå…¨åˆ†æ•£ç§»è¡Œ           :p7, 2026-05, 1M
    æ€§èƒ½æœ€é©åŒ–             :p8, 2026-06, 2M
```

### ãƒ•ã‚§ãƒ¼ã‚º1: çµ±åˆå‹ã§ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ2025å¹´12æœˆ - 2026å¹´1æœˆï¼‰

**ç›®æ¨™**: æ—¢å­˜æŠ€è¡“ã§è¿…é€Ÿã«ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ§‹ç¯‰

**å®Ÿè£…**:
```python
# Lang-Main Node
class ZenohBrainNode:
    def _create_model(self):
        if self.module_type == "lang-main":
            # âœ… æ—¢å­˜ã®MultiModalEvoSpikeNetLMã‚’ä½¿ç”¨
            return MultiModalEvoSpikeNetLM(
                vocab_size=30522,
                d_model=128,
                n_heads=4,
                num_transformer_blocks=4,
                time_steps=10
            )
```

**æˆæœç‰©**:
- âœ… å‹•ä½œã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æ•£è„³ã‚·ã‚¹ãƒ†ãƒ 
- âœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š
- âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š

### ãƒ•ã‚§ãƒ¼ã‚º2: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç§»è¡Œï¼ˆ2026å¹´1æœˆ - 2026å¹´4æœˆï¼‰

**ç›®æ¨™**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰æ®µéšçš„ã«åˆ†æ•£åŒ–

**å„ªå…ˆé †ä½**:
1. **Visual Nodeç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«**ï¼ˆè¨ˆç®—è² è·ãŒæœ€å¤§ï¼‰
2. **Audio Nodeç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«**ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§é‡è¦ï¼‰
3. **Lang Nodeè»½é‡åŒ–**ï¼ˆVisionã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é™¤å»ï¼‰

**å®Ÿè£…ä¾‹**:

```python
# æ–°è¦: SpikingVisionLMï¼ˆVision Nodeå°‚ç”¨ï¼‰
class SpikingVisionLM(nn.Module):
    """
    Visualç‰¹åŒ–SNN LLM
    ç”»åƒç†è§£ã«ç‰¹åŒ–ã—ãŸæ·±å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    """
    def __init__(self, output_dim=128):
        super().__init__()
        # Vision Transformer ãƒ™ãƒ¼ã‚¹ã®SNN
        self.vision_encoder = SpikingVisionTransformer(
            patch_size=16,
            embed_dim=256,
            depth=12,  # æ·±ã„éšå±¤ã§é«˜ç²¾åº¦èªè­˜
            num_heads=8
        )
        
        # é«˜æ¬¡ç‰¹å¾´æŠ½å‡º
        self.feature_processor = SpikingTransformerBlock(
            input_dim=256,
            hidden_dim=512,
            n_heads=8,
            time_steps=20
        )
        
        # æ„å‘³çš„è¡¨ç¾ã¸ã®å¤‰æ›
        self.semantic_layer = nn.Linear(256, output_dim)
    
    def forward(self, image: torch.Tensor):
        """
        Returns:
            high_level_features: æ„å‘³çš„ç‰¹å¾´ï¼ˆSpikeå½¢å¼ï¼‰
            metadata: æ¤œå‡ºç‰©ä½“ã€ä½ç½®æƒ…å ±ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        # Visionå‡¦ç†
        vision_features = self.vision_encoder(image)
        processed = self.feature_processor(vision_features)
        
        # æ„å‘³çš„ç‰¹å¾´æŠ½å‡º
        semantic_features = self.semantic_layer(processed)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç‰©ä½“æ¤œå‡ºã€æ³¨æ„é ˜åŸŸãªã©ï¼‰
        metadata = self._extract_metadata(vision_features)
        
        return semantic_features, metadata
```

**é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«**:

```python
# Visual Node â†’ Lang-Main
visual_packet = {
    "node_id": "visual-0",
    "features": semantic_features,  # é«˜æ¬¡ç‰¹å¾´ï¼ˆ128æ¬¡å…ƒï¼‰
    "metadata": {
        "detected_objects": ["cat", "table"],
        "attention_regions": [[x1,y1,x2,y2], ...],
        "confidence": 0.95
    },
    "timestamp": time.time_ns()
}
comm.publish("visual/features", visual_packet)
```

### ãƒ•ã‚§ãƒ¼ã‚º3: å®Œå…¨åˆ†æ•£åŒ–ï¼ˆ2026å¹´5æœˆ - 2026å¹´8æœˆï¼‰

**ç›®æ¨™**: å…¨ãƒãƒ¼ãƒ‰ãŒç‰¹åŒ–LLMã§çœŸã®åˆ†æ•£è„³ã‚’å®Ÿç¾

**æœ€çµ‚ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```python
# å„ãƒãƒ¼ãƒ‰ã®ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«å®šç¾©
DISTRIBUTED_LLM_CONFIG = {
    "pfc": {
        "model_class": "PFCDecisionEngine",
        "features": [
            "quantum_modulation",
            "attention_routing",
            "working_memory"
        ],
        "size_mb": 100
    },
    "visual": {
        "model_class": "SpikingVisionLM",
        "features": [
            "vision_transformer",
            "object_detection",
            "scene_understanding"
        ],
        "size_mb": 300
    },
    "audio": {
        "model_class": "SpikingAudioLM",
        "features": [
            "speech_recognition",
            "emotion_detection",
            "sound_source_localization"
        ],
        "size_mb": 200
    },
    "lang-main": {
        "model_class": "SpikingTextLM",
        "features": [
            "text_generation",
            "semantic_fusion",
            "context_management"
        ],
        "size_mb": 160
    },
    "motor": {
        "model_class": "SpikingMotorLM",
        "features": [
            "trajectory_planning",
            "motor_consensus",
            "safety_checking"
        ],
        "size_mb": 150
    }
}
```

**PFCçµ±åˆå¼·åŒ–**:

```python
class PFCDecisionEngine:
    """
    å¼·åŒ–ç‰ˆPFC: å„ãƒãƒ¼ãƒ‰ã®LLMã‚’å‹•çš„ã«èª¿æ•´
    """
    def route_with_context(self, input_data):
        """
        é‡å­å¤‰èª¿ã‚’æ´»ç”¨ã—ãŸå‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        """
        # å„ãƒãƒ¼ãƒ‰ã®ç¾åœ¨è² è·ã‚’å–å¾—
        node_status = self.get_node_status()
        
        # Q-PFC: ä¸ç¢ºå®Ÿæ€§ã«åŸºã¥ããƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        uncertainty = self.calculate_uncertainty(input_data)
        
        if uncertainty > threshold:
            # æ¢ç´¢ãƒ¢ãƒ¼ãƒ‰: è¤‡æ•°ãƒãƒ¼ãƒ‰ä¸¦åˆ—å®Ÿè¡Œ
            routes = self.multi_node_exploration(input_data, node_status)
        else:
            # æ´»ç”¨ãƒ¢ãƒ¼ãƒ‰: æœ€é©ãƒãƒ¼ãƒ‰é¸æŠ
            routes = self.optimal_node_selection(input_data, node_status)
        
        return routes
```

---

## å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: çµ±åˆå‹åŸºç›¤ï¼ˆ1-2ãƒ¶æœˆï¼‰

**ã‚¿ã‚¹ã‚¯**:

- [x] âœ… MultiModalEvoSpikeNetLMå®Ÿè£…ï¼ˆæ—¢å­˜ï¼‰
- [ ] ğŸ”„ Lang-Mainãƒãƒ¼ãƒ‰ã¸ã®MultiModalLLMçµ±åˆ
- [ ] ğŸ“‹ æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨ˆæ¸¬
- [ ] ğŸ“‹ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**æˆæœç‰©**:
```
docs/
  â””â”€â”€ MULTIMODAL_BASELINE_BENCHMARK.md
examples/
  â””â”€â”€ run_zenoh_with_multimodal.py
```

### Phase 2: Visionç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆ1.5ãƒ¶æœˆï¼‰

**ã‚¿ã‚¹ã‚¯**:

- [ ] ğŸ“‹ SpikingVisionLMè¨­è¨ˆ
- [ ] ğŸ“‹ Vision Transformer SNNå®Ÿè£…
- [ ] ğŸ“‹ Visual Nodeçµ±åˆ
- [ ] ğŸ“‹ Zenohé€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«æ›´æ–°

**æˆæœç‰©**:
```
evospikenet/
  â””â”€â”€ vision_lm.py          # æ–°è¦: SpikingVisionLM
examples/
  â””â”€â”€ train_vision_lm.py    # æ–°è¦: Visionå­¦ç¿’
tests/
  â””â”€â”€ test_vision_lm.py     # æ–°è¦: ãƒ†ã‚¹ãƒˆ
```

### Phase 3: Audioç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆ1ãƒ¶æœˆï¼‰

**ã‚¿ã‚¹ã‚¯**:

- [ ] ğŸ“‹ SpikingAudioLMè¨­è¨ˆ
- [ ] ğŸ“‹ Speech/Soundå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- [ ] ğŸ“‹ Audio Nodeçµ±åˆ

**æˆæœç‰©**:
```
evospikenet/
  â””â”€â”€ audio_lm.py           # æ–°è¦: SpikingAudioLM
```

### Phase 4: PFCå¼·åŒ–ï¼ˆ1ãƒ¶æœˆï¼‰

**ã‚¿ã‚¹ã‚¯**:

- [ ] ğŸ“‹ PFCDecisionEngineã®å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å¼·åŒ–
- [ ] ğŸ“‹ ãƒãƒ¼ãƒ‰è² è·åˆ†æ•£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- [ ] ğŸ“‹ é‡å­å¤‰èª¿ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ«ãƒãƒãƒ¼ãƒ‰æ¢ç´¢

**æˆæœç‰©**:
```
evospikenet/
  â””â”€â”€ pfc_advanced.py       # å¼·åŒ–ç‰ˆPFC
```

### Phase 5: å®Œå…¨çµ±åˆï¼ˆ1-2ãƒ¶æœˆï¼‰

**ã‚¿ã‚¹ã‚¯**:

- [ ] ğŸ“‹ å…¨ãƒãƒ¼ãƒ‰ç‰¹åŒ–LLMçµ±åˆ
- [ ] ğŸ“‹ ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
- [ ] ğŸ“‹ æ€§èƒ½æœ€é©åŒ–
- [ ] ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

**æˆæœç‰©**:
```
docs/
  â””â”€â”€ DISTRIBUTED_LLM_GUIDE.md
  â””â”€â”€ DEPLOYMENT_GUIDE.md
```

---

## æŠ€è¡“çš„è©³ç´°

### é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«è¨­è¨ˆ

#### é«˜æ¬¡ç‰¹å¾´é€šä¿¡

```python
from dataclasses import dataclass
import torch

@dataclass
class HighLevelFeaturePacket:
    """
    ãƒãƒ¼ãƒ‰é–“ã§é€ä¿¡ã•ã‚Œã‚‹é«˜æ¬¡ç‰¹å¾´ãƒ‘ã‚±ãƒƒãƒˆ
    """
    node_id: str
    modality: str                    # "visual", "audio", "text"
    features: torch.Tensor           # ã‚¹ãƒ‘ã‚¤ã‚¯ç‰¹å¾´ï¼ˆåœ§ç¸®æ¸ˆã¿ï¼‰
    metadata: dict                   # ãƒ¡ã‚¿æƒ…å ±
    timestamp_ns: int                # PTPåŒæœŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    confidence: float                # ä¿¡é ¼åº¦
    
    def serialize(self) -> bytes:
        """Zenohé€ä¿¡ç”¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return pickle.dumps({
            "node_id": self.node_id,
            "modality": self.modality,
            "features": self.features.cpu().numpy(),
            "metadata": self.metadata,
            "timestamp_ns": self.timestamp_ns,
            "confidence": self.confidence
        })
```

#### Zenohãƒˆãƒ”ãƒƒã‚¯è¨­è¨ˆï¼ˆåˆ†æ•£å‹ï¼‰

```
evospikenet/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ visual/high_level      # Visual â†’ Lang/PFC
â”‚   â”œâ”€â”€ audio/high_level       # Audio â†’ Lang/PFC
â”‚   â””â”€â”€ text/high_level        # Lang â†’ PFC
â”œâ”€â”€ routing/
â”‚   â”œâ”€â”€ pfc/decision           # PFC â†’ All Nodes
â”‚   â””â”€â”€ pfc/feedback           # All Nodes â†’ PFC
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ visual/update          # ãƒ¢ãƒ‡ãƒ«æ›´æ–°é€šçŸ¥
â”‚   â”œâ”€â”€ audio/update
â”‚   â””â”€â”€ lang/update
â””â”€â”€ health/
    â””â”€â”€ {node_id}/status       # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
```

### ãƒ¢ãƒ‡ãƒ«åœ§ç¸®ãƒ»æœ€é©åŒ–

#### é‡å­åŒ–ï¼ˆFP16 â†’ INT8ï¼‰

```python
import torch.quantization as quant

def quantize_spiking_model(model: nn.Module):
    """
    SNNãƒ¢ãƒ‡ãƒ«ã‚’INT8ã«é‡å­åŒ–
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’1/4ã«å‰Šæ¸›
    """
    model.qconfig = quant.get_default_qconfig('fbgemm')
    model_prepared = quant.prepare(model)
    
    # Calibrationï¼ˆè¼ƒæ­£ï¼‰
    with torch.no_grad():
        for data in calibration_dataset:
            model_prepared(data)
    
    model_quantized = quant.convert(model_prepared)
    return model_quantized
```

**åŠ¹æœ**:
- ãƒ¡ãƒ¢ãƒª: 300MB â†’ 75MBï¼ˆSpikingVisionLMï¼‰
- æ¨è«–é€Ÿåº¦: 1.5-2å€é«˜é€ŸåŒ–
- ç²¾åº¦åŠ£åŒ–: <2%ï¼ˆSNNã¯é›¢æ•£ã‚¹ãƒ‘ã‚¤ã‚¯ã®ãŸã‚å½±éŸ¿å°ï¼‰

#### ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæåˆˆã‚Šï¼‰

```python
import torch.nn.utils.prune as prune

def prune_spiking_model(model: nn.Module, amount=0.3):
    """
    æ§‹é€ åŒ–ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–
    """
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
        elif isinstance(module, nn.Conv2d):
            prune.ln_structured(module, name='weight', 
                              amount=amount, n=2, dim=0)
    
    return model
```

**åŠ¹æœ**:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 30%å‰Šæ¸›
- ç²¾åº¦åŠ£åŒ–: <3%
- æ¨è«–é€Ÿåº¦: 1.2å€é«˜é€ŸåŒ–

---

## æ¨å¥¨æœ€çµ‚æ§‹æˆ

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

```yaml
distributed_brain:
  architecture: "hybrid_to_distributed"
  
  nodes:
    pfc:
      model: "PFCDecisionEngine"
      hardware: "CPU (4 cores, 2GB RAM)"
      location: "Central Server"
      responsibilities:
        - "Quantum-modulated routing"
        - "Working memory management"
        - "Global coordination"
    
    visual:
      model: "SpikingVisionLM"
      hardware: "GPU (NVIDIA Jetson Xavier, 8GB)"
      location: "Edge Device 1"
      responsibilities:
        - "Real-time vision processing"
        - "Object detection & tracking"
        - "Scene understanding"
    
    audio:
      model: "SpikingAudioLM"
      hardware: "GPU (NVIDIA Jetson Nano, 4GB)"
      location: "Edge Device 2"
      responsibilities:
        - "Speech recognition"
        - "Sound event detection"
        - "Emotion recognition"
    
    lang-main:
      model: "SpikingTextLM"
      hardware: "GPU (NVIDIA RTX 3060, 12GB)"
      location: "Central Server"
      responsibilities:
        - "Text generation"
        - "Semantic integration"
        - "Response synthesis"
    
    motor:
      model: "SpikingMotorLM"
      hardware: "EdgeTPU (Google Coral)"
      location: "Robot Controller"
      responsibilities:
        - "Motor planning"
        - "Consensus control"
        - "Safety validation"
  
  communication:
    protocol: "Zenoh"
    qos: "Best-effort for spikes, Reliable for features"
    compression: "Enabled (zstd)"
    encryption: "TLS 1.3 (production)"
```

### é–‹ç™ºå„ªå…ˆé †ä½

**If çŸ­æœŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆ<3ãƒ¶æœˆï¼‰**:
â†’ ğŸŸ¢ **çµ±åˆå‹ï¼ˆMultiModalEvoSpikeNetLMï¼‰**ã‚’æ¨å¥¨

**If é‡ç”£ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ6ãƒ¶æœˆä»¥ä¸Šï¼‰**:
â†’ ğŸ”´ **åˆ†æ•£å‹ï¼ˆç‰¹åŒ–LLMsï¼‰**ã‚’å¼·ãæ¨å¥¨

**If ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
â†’ ğŸŸ¡ **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**ï¼ˆä¸¡æ–¹å®Ÿè£…ã—ã¦æ¯”è¼ƒï¼‰ã‚’æ¨å¥¨

---

## ã¾ã¨ã‚

### æ„æ€æ±ºå®šãƒãƒˆãƒªã‚¯ã‚¹

| åˆ¤æ–­åŸºæº–               | çµ±åˆå‹ã‚¹ã‚³ã‚¢ | åˆ†æ•£å‹ã‚¹ã‚³ã‚¢ | æ¨å¥¨   |
| ---------------------- | ------------ | ------------ | ------ |
| **çŸ­æœŸé–‹ç™ºé€Ÿåº¦**       | 9/10         | 4/10         | çµ±åˆå‹ |
| **é•·æœŸä¿å®ˆæ€§**         | 5/10         | 9/10         | åˆ†æ•£å‹ |
| **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**   | 4/10         | 10/10        | åˆ†æ•£å‹ |
| **æ€§èƒ½ï¼ˆè¤‡é›‘ã‚¿ã‚¹ã‚¯ï¼‰** | 6/10         | 9/10         | åˆ†æ•£å‹ |
| **ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡**       | 5/10         | 9/10         | åˆ†æ•£å‹ |
| **éšœå®³è€æ€§**           | 3/10         | 9/10         | åˆ†æ•£å‹ |
| **å®Ÿè£…è¤‡é›‘åº¦**         | 9/10         | 5/10         | çµ±åˆå‹ |

### æœ€çµ‚æ¨å¥¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  ğŸ¯ æ¨å¥¨æˆ¦ç•¥: æ®µéšçš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ              â”‚
â”‚                                                         â”‚
â”‚  Phase 1 (ç¾åœ¨-2026å¹´1æœˆ):                             â”‚
â”‚    âœ… MultiModalEvoSpikeNetLMã§è¿…é€Ÿã«ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—      â”‚
â”‚                                                         â”‚
â”‚  Phase 2 (2026å¹´2æœˆ-4æœˆ):                              â”‚
â”‚    ğŸ”„ Vision/Audioç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã«æ®µéšçš„ç§»è¡Œ               â”‚
â”‚                                                         â”‚
â”‚  Phase 3 (2026å¹´5æœˆ-8æœˆ):                              â”‚
â”‚    ğŸš€ å®Œå…¨åˆ†æ•£å‹ã§2026å¹´é‡ç”£ãƒ­ãƒœãƒƒãƒˆã«å¯¾å¿œ             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç†ç”±**:
1. çŸ­æœŸçš„ã«ã¯çµ±åˆå‹ã§è¿…é€Ÿãªæˆæœå‰µå‡º
2. æ®µéšçš„ç§»è¡Œã§ãƒªã‚¹ã‚¯æœ€å°åŒ–
3. é•·æœŸçš„ã«ã¯åˆ†æ•£å‹ã§çœŸã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å®Ÿç¾
4. 2026å¹´é‡ç”£ãƒ­ãƒœãƒƒãƒˆè¦ä»¶ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã€éšœå®³è€æ€§ï¼‰ã‚’æº€ãŸã™

---

## å‚è€ƒè³‡æ–™

- `evospikenet/models.py`: MultiModalEvoSpikeNetLMå®Ÿè£…
- `evospikenet/vision.py`: SpikingVisionEncoder
- `evospikenet/audio.py`: SpikingAudioEncoder
- `examples/run_zenoh_distributed_brain.py`: åˆ†æ•£è„³ã‚·ã‚¹ãƒ†ãƒ 
- `docs/DISTRIBUTED_BRAIN_SYSTEM.md`: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°
- `docs/SPIKE_COMMUNICATION_ANALYSIS.md`: é€šä¿¡åˆ†æ

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¬ãƒ“ãƒ¥ãƒ¼
2. ğŸ“‹ Phase 1å®Ÿè£…è¨ˆç”»ã®æ‰¿èª
3. ğŸ”§ MultiModalLLMã®Lang-Mainçµ±åˆ
4. ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š
5. ğŸš€ Phase 2ã¸ã®ç§»è¡Œåˆ¤æ–­
