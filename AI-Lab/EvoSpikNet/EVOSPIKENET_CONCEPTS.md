# Copyright 2025 Moonlight Technologies Inc.. All Rights Reserved.
# Auth Masahiro Aoki


# EvoSpikeNet: 主要な概念

**最終更新日:** 2025年11月26日

このドキュメントでは、EvoSpikeNetフレームワークの中核をなす、より高度でユニークな概念に関する技術的な詳細を説明します。

---

## 1. SpikingEvoSpikeNetLM (`evospikenet/models.py`)

`SpikingEvoSpikeNetLM`は、EvoSpikeNetのフラッグシップ言語モデルであり、標準的なTransformerアーキテクチャとスパイクニューラルネットワーク（SNN）の原理を融合させたハイブリッドモデルです。

### 1.1. アーキテクチャの概要
- **`TAS-Encoding` (`evospikenet/encoding.py`)**: Time-Adaptive Spikeエンコーディング層。テキストトークンを、その重要性や文脈に応じて時間的に適応するスパイク列に変換します。
- **`SpikingTransformerBlock`**: モデルのコアビルディングブロック。スパイクベースの自己アテンションとフィードフォワードネットワークをカプセル化しています。
- **`ChronoSpikeAttention` (`evospikenet/attention.py`)**: `SpikingTransformerBlock`内で使用されるアテンション機構。スパイク間の時間的な関係を捉え、文脈ベクトルを生成します。

### 1.2. 動作原理
入力テキストはトークン化され、`TAS-Encoding`層によってスパイク列に変換されます。その後、`SpikingTransformerBlock`のスタックを通過し、最終的に次のトークンを予測するためにデコードされます。このアーキテクチャにより、モデルは従来の言語モデルの強力なシーケンス処理能力と、SNNの持つ時間的ダイナミクスやエネルギー効率といった利点を組み合わせることができます。

---

## 2. MultiModalEvoSpikeNetLM (`evospikenet/models.py`)

`MultiModalEvoSpikeNetLM`は、EvoSpikeNetの能力をテキストだけでなく、画像や音声を含むトライモーダル（3つのモダリティ）データへと拡張します。

### 2.1. アーキテクチャの概要
- **`SpikingVisionEncoder` (`evospikenet/vision.py`)**: スパイクベースの畳み込みニューラルネットワーク（SCNN）を使用して画像から特徴を抽出します。
- **`SpikingAudioEncoder` (`evospikenet/audio.py`)**: スパイクベースの音声処理モデルを使用して、音声波形データから特徴を抽出します。
- **テキストエンコーダ**: テキストプロンプトから特徴を抽出します。
- **融合メカニズム**: 3つのモダリティから抽出された特徴ベクトルは、クロスアテンションなどのメカニズムを介して融合され、リッチな統合表現を生成します。

---

## 3. ハイブリッド検索RAG (`evospikenet/rag_milvus.py`)

Retrieval-Augmented Generation (RAG) システムは、ベクトル検索とキーワード検索を組み合わせたハイブリッド検索アーキテクチャを実装しています。

### 3.1. アーキテクチャ
- **Milvus (ベクトル検索)**: セマンティック（意味的）な類似性検索を担当します。
- **Elasticsearch (キーワード検索)**: 伝統的なフルテキスト検索を担当します。

### 3.2. ワークフロー
1.  **並列検索**: ユーザーのクエリは、MilvusとElasticsearchに同時に送信されます。
2.  **結果の融合**: 両方の結果は、**Reciprocal Rank Fusion (RRF)** アルゴリズムを使用してインテリジェントに融合されます。
3.  **データ同期**: 知識ベースに対するCRUD（作成、更新、削除）操作は、両方のデータベースに対して同期的に実行され、データの一貫性を保証します。

---

## 4. フェデレーテッド学習 (`evospikenet/federated.py`)

`Flower`（`flwr`）フレームワークを統合することで、データプライバシーを保護しながら分散環境でモデルを協調的に訓練するフェデレーテッド学習をサポートします。

### 4.1. コンポーネント
- **`EvoSpikeNetClient`**: `flwr.client.NumPyClient`を継承し、ローカルでのモデル訓練ロジックをカプセル化します。
- **`DistributedBrainClient`**: 分散脳アーキテクチャに特化したカスタムクライアント。パラメータの直接平均化の代わりに、**スパイク蒸留**（平均発火活動の送信）を用いて知識を共有します。

---

## 5. RESTful API (`evospikenet/api.py`)

`FastAPI`をベースにしたRESTful APIは、外部アプリケーションがEvoSpikeNetの機能にプログラムでアクセスするための主要なインターフェースです。

- **主要エンドポイント**:
    - `/api/generate`: テキスト生成を実行します。
    - `/api/distributed_brain/status`: 分散脳シミュレーションのリアルタイム状態を取得します。
    - `/api/distributed_brain/prompt`: シミュレーションに新しいプロンプト（テキスト、画像、音声）を送信します。
    - `/api/distributed_brain/result`: シミュレーションの最終結果を取得します。
- **モデルのライフサイクル**: APIサーバーは起動時に、最新の訓練済みモデルを一度だけメモリに読み込み、リクエストごとの処理遅延を最小限に抑えます。

---

## 6. 分散脳アーキテクチャ (`examples/run_distributed_brain_simulation.py`)

分散脳アーキテクチャは、生物学的な脳の機能的専門化に着想を得た、フレームワークの最も高度な機能です。このモデルでは、専門化された「機能モジュール」が`torch.distributed`を用いて個別のプロセスとして実行され、それらを中央の「前頭前野（PFC）モジュール」が調整する、マスター/スレーブ型のアーキテクチャを採用しています。

### 6.1. アーキテクチャの構成要素

-   **マスタープロセス (Rank 0): PFC (Prefrontal Cortex) モジュール (`evospikenet/pfc.py`)**:
    -   **役割**: アーキテクチャの「認知制御センター」として機能するマスターノード。
    -   **タスクルーティング**: APIからプロンプトを受信し、タスクの内容を解釈して、最適な機能モジュール（スレーブプロセス）に動的にディスパッチします。
    -   **状態管理**: 全シミュレーションの状態を管理し、定期的にFastAPIバックエンドに状態をPOSTします。これにより、Web UI上でリアルタイムのモニタリングが可能になります。
    -   **フラグ制御**: シミュレーションの開始と停止は、ファイルシステム上のフラグ (`/tmp/stop_evospikenet_simulation.flag`) によって制御されます。

-   **スレーブプロセス (Rank > 0): 機能モジュール (`evospikenet/functional_modules.py`)**:
    -   **役割**: 特定のタスク（視覚、言語、運動など）を実行する専門家として機能するスレーブノード。
    -   **階層的構造**: 単純なモジュールではなく、多くの機能は**階層的な処理パイプライン**として構成されます。例えば、「Language Focus」シミュレーションでは、言語の親ノード (Rank 4) が、埋め込み (Rank 7)、TASエンコーディング (Rank 8) などの子ノードを順次呼び出して処理を進めます。同様の階層構造が、視覚、聴覚、運動の各ドメインにも実装されています。
    -   **動的・マルチモーダルモデルローディング**: 各スレーブプロセスは、起動時に`--model-config`引数で指定されたモデルを動的にダウンロードして読み込みます。この際、モデルの`config.json`に`"model_type": "MultiModalEvoSpikeNetLM"`が指定されていれば、そのノードは`MultiModalEvoSpikeNetLM`をインスタンス化します。これにより、例えば視覚モジュールのような専門ノードが、単一の視覚処理だけでなく、テキストプロンプトも同時に解釈する、より高度なマルチモーダルな振る舞いを持つことが可能になります。

### 6.2. プロセス間通信とワークフロー

-   **通信基盤**: `torch.distributed`のPoint-to-Point通信（`send`/`recv`）を用いて、プロセス間の高速で効率的なデータ転送を実現します。PFCとスレーブ間の通信は、送信されるデータの種類（テキスト、画像、音声）を事前に通知する**マニフェストベースのプロトコル**を採用しており、これによりマルチモーダルデータの柔軟な送受信が可能になっています。
-   **ワークフロー**:
    1.  **プロンプト入力**: ユーザーがUIからプロンプトを送信すると、それはFastAPIバックエンドに渡されます。
    2.  **PFCによるポーリング**: マスタープロセス (PFC) は、APIを定期的にポーリングして新しいプロンプトを取得します。
    3.  **タスクディスパッチ**: PFCはプロンプトを処理し、タスクを適切な機能モジュール（または階層の親モジュール）にディスパッチします。
    4.  **階層的処理**: 指示を受けた親モジュールは、必要に応じて子モジュールにサブタスクを送信し、その結果を待ちます。処理は階層内を流れ、最終的な結果が親モジュールに集約されます。
    5.  **結果の返却**: 親モジュールは、集約した結果をPFCに返します。
    6.  **状態と結果の報告**: PFCは、シミュレーションのリアルタイム状態と最終結果をそれぞれ専用のAPIエンドポイントにPOSTします。UIはこれらのエンドポイントをポーリングして、表示を動的に更新します。

-   **デバッグとロギング**: 各プロセスは、自身のランクに応じた個別のログファイル (`/tmp/sim_rank_{rank}.log`) を生成します。これにより、分散環境下での複雑なインタラクションのデバッグが容易になります。
