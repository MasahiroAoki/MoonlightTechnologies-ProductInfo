# Copyright 2025 Moonlight Technologies Inc.. All Rights Reserved.
# Auth Masahiro Aoki

# EvoSpikeNetLM 3Bおよび7Bモデルの実現可能性に関する調査報告書

## 1. 結論

EvoSpikeNetLMのアーキテクチャを3B（30億）および7B（70億）パラメータ規模にスケールアップすることは、**技術的に実現可能**です。

本調査により、モデルの総パラメータ数を決定する主要なハイパーパラメータ（`num_blocks`, `d_model`）を特定し、目標とするモデル規模を達成するための具体的な構成を導き出しました。

## 2. パラメータ計算式

モデルの総パラメータ数は、以下の式によって概算できます。

- `L`: `num_transformer_blocks` (層の数)
- `d`: `d_model` (モデルの次元)
- `V`: `vocab_size` (語彙数、`bert-base-uncased` の場合は 30,522)

**総パラメータ数 ≈ `2 * V * d` + `L * 12 * d^2`**

この式は、以下の要素から構成されています。
- **埋め込み層 & 出力層:** `2 * V * d`
- **Transformerブロック群:** `L * 12 * d^2`
  - 各ブロックには、`ChronoSpikeAttention` (`4 * d^2`) と `SpikingFFN` (`8 * d^2`) が含まれます。

## 3. 推奨モデル構成

上記計算式に基づき、目標規模を達成するためのハイパーパラメータ構成を以下に提案します。

### 3.1. 3B パラメータモデルの候補

| オプション | `num_blocks` (L) | `d_model` (d) | 推定パラメータ数 | 備考 |
| :--- | :--- | :--- | :--- | :--- |
| **1. バランス型** | 32 | 2560 | **~2.7B** | GPT-3 Smallに近い構成 |
| **2. ワイド型** | 32 | 2816 | **~3.2B** | |
| **3. ディープ型** | 40 | 2560 | **~3.3B** | |

### 3.2. 7B パラメータモデルの候補

| オプション | `num_blocks` (L) | `d_model` (d) | 推定パラメータ数 | 備考 |
| :--- | :--- | :--- | :--- | :--- |
| **1. Llama-2 7B風** | 32 | 4096 | **~6.7B** | Llama-2 7Bに近い構成 |
| **2. 大規模版** | 40 | 4096 | **~8.3B** | |

## 4. 実現に向けた課題と考察

これらの大規模モデルの構築は可能ですが、成功させるためには以下の点を考慮する必要があります。

- **計算リソース:**
  - **GPU:** 3Bモデルでも最低A100 80GBが複数台、7Bモデルではさらに多くのGPUクラスタが必要になると予想されます。
  - **データ:** 高品質で大規模なテキストコーパスが不可欠です。
  - **時間:** 学習には数週間から数ヶ月単位の時間がかかる可能性があります。

- **学習の安定性:**
  - 大規模モデルの学習は本質的に不安定になりがちです。学習率のウォームアップとスケジューリング、勾配クリッピング、適切な重みの初期化（例: `xavier`, `kaiming`）などのテクニックが必須となります。

- **SNN特有の課題:**
  - **代理勾配:** `snntorch`で用いられる代理勾配が、これほど大規模なネットワークで安定して機能するかは慎重な検証が必要です。勾配消失や爆発のリスクが高まる可能性があります。
  - **カスタムモジュール:** `MetaSTDP`や`AEG`といった独自の制御モジュールが、大規模な学習プロセスにどのような影響を与えるか（安定させるか、あるいは不安定要因となるか）は未知数であり、小規模な実験からスケールアップさせていくアプローチが推奨されます。

## 5. 総括

EvoSpikeNetLMは、その設計上、大規模言語モデルへのスケールアップが可能なアーキテクチャを持っています。本報告書で提示したモデル構成と課題を考慮することで、3Bおよび7Bモデルの開発に着手することが可能です。
